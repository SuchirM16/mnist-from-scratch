{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "First we need to get the MNIST database onto our computer and put in the right matrix format. The data was acquired from the cvdfoundation GitHub repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "\n",
    "def load_images(filename):\n",
    "    with open(filename, \"rb\") as f: # autocloses the file after reading vs:f = open(...), data = f.read(), f.close()\n",
    "        _, num, rows, cols = struct.unpack(\">IIII\", f.read(16)) # Read 16 bytes of metadata, put 4 in each, \"_\" is a throwaway magic number in the dataset \n",
    "        data = np.frombuffer(f.read(), dtype=np.uint8) # read pixel data as byte-values, iterates automatically through file\n",
    "        return data.reshape(num, rows, cols) # reshape to (num_samples, 28, 28)\n",
    "\n",
    "def load_labels(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        _, num = struct.unpack(\">II\", f.read(8)) # Read metadata from labels file\n",
    "        data = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining What's Going on\n",
    "\n",
    "Data is a numpy ndarray in this case, which is a multidimensional array. After we load all of the bytes in a flattened 1D array, we use the reshape function to create a 3D array where there are 60,000 cells of 28 * 28 pixel images. \n",
    "\n",
    "The data in the next function with the labels doesn't need to be reshapen because it's supposed to be a 1D array with only byte values for each integer from 0 to 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'ml (Python -1.-1.-1)' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "x_train = load_images(\"train-images.idx3-ubyte\")\n",
    "y_train = load_labels(\"train-labels.idx1-ubyte\")\n",
    "\n",
    "x_test = load_images(\"t10k-images.idx3-ubyte\")\n",
    "y_test = load_labels(\"t10k-labels.idx1-ubyte\")\n",
    "\n",
    "print(\"MNIST dataset successfully loaded into numpy arrays.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the common function notation y = f(x), the input data is named x while the output labels are named y.\n",
    "\n",
    "## Preprocessing the data\n",
    "\n",
    "### Flattening the image array\n",
    "The next step is to flatten the array back into a 784 dimensional vector and normalize the values of each pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], 28 * 28)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28 * 28)\n",
    "\n",
    "# We test to make sure the arrays are the right shape now\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the array values\n",
    "Next we need to normalize all of the values in the array so that they are between 0 and 1 instead of 0 to 255. This isn't always done but we do it on this perceptron so we can measure a neuron's activation more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1.0\n",
      "0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.astype(np.float32) / 255.0\n",
    "x_test = x_test.astype(np.float32) / 255.0\n",
    "\n",
    "# Testing to see that it's properly normalized\n",
    "print(x_train.min(), x_train.max())  # Should print: 0.0 1.0\n",
    "print(x_test.min(), x_test.max())    # Should print: 0.0 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This typecasts the uint8 values from the array into float32. Now we can normalize this by dividing it by the max value - 255. Now every value will be a ratio compared to 255, leaving a value from 0 to 1.\n",
    "\n",
    "### One-hot Encoding the labels\n",
    "After flattening the arrays, we need to one-hot encode each digit. \n",
    "\n",
    "0 -> 1000000000\n",
    "\n",
    "1 -> 0100000000\n",
    "\n",
    "2 -> 0010000000\n",
    "\n",
    "...and so on. We do this because we only want one output neuron firing for each digit in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "(10000, 10)\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(y, num_classes=10):\n",
    "    return np.eye(num_classes)[y]\n",
    "\n",
    "y_train = one_hot_encode(y_train)\n",
    "y_test = one_hot_encode(y_test)\n",
    "\n",
    "# Verify the shape of our array and its formatting\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Network Architecture\n",
    "\n",
    "Now that all of the preprocessing steps are done, we need to initialize the neural network. In this case, we will make one with two hidden layers consisting of 128 neurons, an input layer with 784 neurons, and an output with 10 neurons - one for each digit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 shape: (784, 128)\n",
      "b1 shape: (1, 128)\n",
      "W2 shape: (128, 64)\n",
      "b2 shape: (1, 64)\n",
      "W3 shape: (64, 10)\n",
      "b3 shape: (1, 10)\n"
     ]
    }
   ],
   "source": [
    "# Define the sizes of the layers\n",
    "\n",
    "input_size = 784\n",
    "hidden_size1 = 128\n",
    "hidden_size2 = 64\n",
    "output_size = 10\n",
    "\n",
    "#Initialize the weights and biases with random values\n",
    "np.random.seed(42)\n",
    "\n",
    "W1 = np.random.randn(input_size, hidden_size1) * 0.01 # (784, 128)\n",
    "b1 = np.zeros((1, hidden_size1)) \n",
    "\n",
    "W2 = np.random.randn(hidden_size1, hidden_size2) * 0.01\n",
    "b2 = np.zeros((1, hidden_size2))\n",
    "\n",
    "W3 = np.random.randn(hidden_size2, output_size) * 0.01\n",
    "b3 = np.zeros((1, output_size))\n",
    "\n",
    "# Verify everything\n",
    "print(\"W1 shape:\", W1.shape)  # (784, 128)\n",
    "print(\"b1 shape:\", b1.shape)  # (1, 128)\n",
    "\n",
    "print(\"W2 shape:\", W2.shape)  # (128, 64)\n",
    "print(\"b2 shape:\", b2.shape)  # (1, 64)\n",
    "\n",
    "print(\"W3 shape:\", W3.shape)  # (64, 10)\n",
    "print(\"b3 shape:\", b3.shape)  # (1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "\n",
    "With all the weights randomly initialized, we now do a forward pass of our neural network in its current state and see what we get.\n",
    "\n",
    "Forward propagation is just the process of passing the input at each payer through the weights and biases to get the outputs. \n",
    "\n",
    "This can be described with the function Z = W*X + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining activation functions for our layers\n",
    "\n",
    "def relu(z): # Non linear activation function for hidden layers\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def softmax(z): # Used for the output layer to turn the outputs into a probability from 0 to 1\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resources to find out more about these activation functions:\n",
    "\n",
    "[reLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))\n",
    "\n",
    "[Softmax](https://en.wikipedia.org/wiki/Softmax_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions (Softmax Output):\n",
      " [[0.09999784 0.1000113  0.09999249 0.10004964 0.09998993 0.10003599\n",
      "  0.09999102 0.09996785 0.09998571 0.09997824]\n",
      " [0.09992773 0.1000102  0.1000791  0.10006082 0.09997785 0.10000442\n",
      "  0.10006154 0.10000583 0.0999256  0.0999469 ]\n",
      " [0.10004008 0.09996258 0.10000727 0.10001555 0.10002374 0.09999946\n",
      "  0.10001566 0.10000466 0.099969   0.09996199]\n",
      " [0.09998447 0.09997845 0.10005079 0.10003316 0.10002571 0.09999959\n",
      "  0.09999116 0.10000886 0.09996861 0.09995919]\n",
      " [0.10003088 0.09999359 0.09996209 0.10001275 0.10003859 0.10001534\n",
      "  0.09995681 0.10003345 0.09996493 0.09999156]]\n",
      "Sum of each row (should be 1): [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "def forward_propagation(x):\n",
    "    global W1, b1, W2, b2, W3, b3 # Use the globally defined values for these variables\n",
    "\n",
    "    # Pass through 1st hidden layer\n",
    "    Z1 = np.dot(x, W1) + b1 # Pass input layer through weights -> take the dot product of both matrices then add bias\n",
    "    A1 = relu(Z1) # Pass the resulting matrix through activation function\n",
    "\n",
    "    # Do the same for the 2nd hidden layer\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = relu(Z2)\n",
    "\n",
    "    # Output layer\n",
    "\n",
    "    Z3 = np.dot(A2, W3) + b3\n",
    "    A3 = softmax(Z3) # The output uses a different activation function\n",
    "\n",
    "    return Z1, A1, Z2, A2, Z3, A3 # Return all intermediate values for backpropagation\n",
    "\n",
    "# Running forward propagation:\n",
    "\n",
    "batch_X = x_train[:5]  # Take first 5 images\n",
    "_, _, _, _, _, predictions = forward_propagation(batch_X)\n",
    "\n",
    "print(\"Predictions (Softmax Output):\\n\", predictions)\n",
    "print(\"Sum of each row (should be 1):\", np.sum(predictions, axis=1))  # Verify softmax sums to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Loss Function\n",
    "\n",
    "We use the labels for the dataset to compute a loss function to quantize how off we were. We're computing the cross entropy loss in this case, find more info [here](https://en.wikipedia.org/wiki/Cross-entropy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Entropy Loss: 2.302670109823364\n"
     ]
    }
   ],
   "source": [
    "def compute_loss(y_true, y_pred):\n",
    "    m = y_true.shape[0] # number of samples in batch\n",
    "    loss = -np.sum(y_true * np.log(y_pred + 1e-8)) / m # Add epsilon to avoid log(0)\n",
    "    return loss\n",
    "\n",
    "# Running the loss function\n",
    "\n",
    "batch_X = x_train[:5]  # First 5 images\n",
    "batch_Y = y_train[:5]  # First 5 labels\n",
    "\n",
    "_, _, _, _, _, predictions = forward_propagation(batch_X)  # Get predicted probabilities\n",
    "\n",
    "loss = compute_loss(batch_Y, predictions)\n",
    "print(\"Cross-Entropy Loss:\", loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "Backpropagation is how you compute gradients to see how much each weight affected the output, and you use this to update the weights. \n",
    "\n",
    "Computing the gradients involves basic partial derivatives and chain rule, and more information on the math can be found [here](https://en.wikipedia.org/wiki/Backpropagation).\n",
    "\n",
    "We are trying to calculate the gradient of the Loss function with respect to W.\n",
    "\n",
    "Keep in mind that\n",
    "$$\n",
    "Loss = L(A(Z(W)))\n",
    "$$\n",
    "\n",
    "so calculating the gradient for weights involves simple chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial A} \\cdot \\frac{\\partial A}{\\partial Z} \\cdot \\frac{\\partial Z}{\\partial W}\n",
    "$$\n",
    "\n",
    "Where L is the loss function, A is the activation function of a layer, Z is the linear transformation Z = Wx + b, and W is the weight. Everything is computed with respect to W basically.\n",
    "\n",
    "Backpropagation for biases: \n",
    "$$\n",
    "\\frac{\\partial Z}{\\partial b} = 1 \n",
    "$$\n",
    "because biases don't depend on the input like Wx does.\n",
    "\n",
    "Thus, the gradient of the biases is:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial Z}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_derivative(Z):\n",
    "    \"\"\"Derivative of ReLU function\"\"\"\n",
    "    return Z > 0  # Returns 1 for positive values, 0 otherwise\n",
    "\n",
    "def backward_propagation(X, Y, Z1, A1, Z2, A2, Z3, A3):\n",
    "    \"\"\"\n",
    "    Computes the gradients using backpropagation.\n",
    "    X: Input data\n",
    "    Y: True labels (one-hot encoded)\n",
    "    Z1, A1, Z2, A2, Z3, A3: Values from forward propagation\n",
    "    \"\"\"\n",
    "    global W1, b1, W2, b2, W3, b3  # Use global weights and biases\n",
    "    \n",
    "    m = X.shape[0]  # Batch size\n",
    "\n",
    "    # Compute gradient for Output Layer (Softmax & Cross-Entropy Loss)\n",
    "    dZ3 = A3 - Y  # Gradient of loss with respect to Z3\n",
    "    dW3 = np.dot(A2.T, dZ3) / m  # Gradient of loss w.r.t W3\n",
    "    db3 = np.sum(dZ3, axis=0, keepdims=True) / m  # Gradient of loss w.r.t b3\n",
    "\n",
    "    # Compute gradient for Hidden Layer 2 (ReLU)\n",
    "    dA2 = np.dot(dZ3, W3.T)  # Backpropagate through W3\n",
    "    dZ2 = dA2 * relu_derivative(Z2)  # Apply ReLU derivative\n",
    "    dW2 = np.dot(A1.T, dZ2) / m  # Gradient of loss w.r.t W2\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m  # Gradient of loss w.r.t b2\n",
    "\n",
    "    # Compute gradient for Hidden Layer 1 (ReLU)\n",
    "    dA1 = np.dot(dZ2, W2.T)  # Backpropagate through W2\n",
    "    dZ1 = dA1 * relu_derivative(Z1)  # Apply ReLU derivative\n",
    "    dW1 = np.dot(X.T, dZ1) / m  # Gradient of loss w.r.t W1\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m  # Gradient of loss w.r.t b1\n",
    "\n",
    "    return dW1, db1, dW2, db2, dW3, db3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to test our backpropagation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW1 shape: (784, 128)\n",
      "db1 shape: (1, 128)\n",
      "dW2 shape: (128, 64)\n",
      "db2 shape: (1, 64)\n",
      "dW3 shape: (64, 10)\n",
      "db3 shape: (1, 10)\n"
     ]
    }
   ],
   "source": [
    "batch_X = x_train[:5]  # First 5 images\n",
    "batch_Y = y_train[:5]  # First 5 labels\n",
    "\n",
    "# Forward propagation\n",
    "Z1, A1, Z2, A2, Z3, A3 = forward_propagation(batch_X)\n",
    "\n",
    "# Compute gradients\n",
    "dW1, db1, dW2, db2, dW3, db3 = backward_propagation(batch_X, batch_Y, Z1, A1, Z2, A2, Z3, A3)\n",
    "\n",
    "# Print gradient shapes to verify\n",
    "print(\"dW1 shape:\", dW1.shape)  # Expected: (784, 128)\n",
    "print(\"db1 shape:\", db1.shape)  # Expected: (1, 128)\n",
    "print(\"dW2 shape:\", dW2.shape)  # Expected: (128, 64)\n",
    "print(\"db2 shape:\", db2.shape)  # Expected: (1, 64)\n",
    "print(\"dW3 shape:\", dW3.shape)  # Expected: (64, 10)\n",
    "print(\"db3 shape:\", db3.shape)  # Expected: (1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Weights\n",
    "\n",
    "Now we use the computed gradients to nudge the weights using gradient descent.\n",
    "\n",
    "The gradient descent algorithm looks like this:\n",
    "\n",
    "$$\n",
    "{W} = {W} - {\\alpha}\\frac{\\partial L}{\\partial W}\n",
    "$$\n",
    "\n",
    "and we do the same for the biases:\n",
    "$$\n",
    "{b} = {b} - {\\alpha}\\frac{\\partial L}{\\partial b}\n",
    "$$\n",
    "\n",
    "Where alpha is the learning rate, or how big the increments to W are. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(dW1, db1, dW2, db2, dW3, db3, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    Updates weights and biases using gradient descent.\n",
    "    dW1, db1, dW2, db2, dW3, db3: Gradients from backpropagation\n",
    "    learning_rate: Step size for updates\n",
    "    \"\"\"\n",
    "    global W1, b1, W2, b2, W3, b3  # Access global weights and biases\n",
    "\n",
    "    # Update weights and biases using gradient descent\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    W3 -= learning_rate * dW3\n",
    "    b3 -= learning_rate * db3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we test this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 change: 0.020560581875656167\n"
     ]
    }
   ],
   "source": [
    "# Take a batch of 5 images\n",
    "batch_X = x_train[:5]\n",
    "batch_Y = y_train[:5]\n",
    "\n",
    "# Forward propagation\n",
    "Z1, A1, Z2, A2, Z3, A3 = forward_propagation(batch_X)\n",
    "\n",
    "# Compute gradients\n",
    "dW1, db1, dW2, db2, dW3, db3 = backward_propagation(batch_X, batch_Y, Z1, A1, Z2, A2, Z3, A3)\n",
    "\n",
    "# Store original W1 for comparison\n",
    "W1_before = W1.copy()\n",
    "\n",
    "# Update weights\n",
    "update_parameters(dW1, db1, dW2, db2, dW3, db3, learning_rate=0.01)\n",
    "\n",
    "# Compare before and after update\n",
    "print(\"W1 change:\", np.sum(np.abs(W1 - W1_before)))  # Should be non-zero if weights updated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training for multiple epochs\n",
    "\n",
    "Now that all of our functions are working, the next step is to actually train the model for multiple epochs and go through all of the data. \n",
    "\n",
    "An epoch is one full pass through the data set, and we will be training the model for multiple epochs.\n",
    "\n",
    "For each epoch, we:\n",
    "\n",
    "1. Loop through all of our mini-batches of data \n",
    "2. Perform forward propagation and compute predictions\n",
    "3. Compute the loss\n",
    "4. Perform backpropagation\n",
    "5. Update all of the weights using gradient descent\n",
    "\n",
    "And then we repeat this for multiple epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x_train, y_train, epochs=10, batch_size=32, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    Trains the neural network using mini-batch gradient descent.\n",
    "    \n",
    "    x_train: Training images (flattened)\n",
    "    y_train: One-hot encoded labels\n",
    "    epochs: Number of times the model sees the full dataset\n",
    "    batch_size: Number of samples processed at a time\n",
    "    learning_rate: Step size for weight updates\n",
    "    \"\"\"\n",
    "    num_samples = x_train.shape[0]  # Number of training samples\n",
    "    num_batches = num_samples // batch_size  # Total batches per epoch\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0  # Track total loss for the epoch\n",
    "        \n",
    "        # Shuffle the training data so the model doesn't learn patterns based on the order of the data\n",
    "        indices = np.random.permutation(num_samples)\n",
    "        x_train_shuffled = x_train[indices]\n",
    "        y_train_shuffled = y_train[indices]\n",
    "\n",
    "        # Process mini-batches\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            batch_X = x_train_shuffled[i:i + batch_size]\n",
    "            batch_Y = y_train_shuffled[i:i + batch_size]\n",
    "\n",
    "            # Forward propagation\n",
    "            Z1, A1, Z2, A2, Z3, A3 = forward_propagation(batch_X)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = compute_loss(batch_Y, A3)\n",
    "            total_loss += loss  # Accumulate loss over all batches\n",
    "\n",
    "            # Backpropagation\n",
    "            dW1, db1, dW2, db2, dW3, db3 = backward_propagation(batch_X, batch_Y, Z1, A1, Z2, A2, Z3, A3)\n",
    "\n",
    "            # Update parameters\n",
    "            update_parameters(dW1, db1, dW2, db2, dW3, db3, learning_rate)\n",
    "\n",
    "        # Print progress at the end of each epoch\n",
    "        avg_loss = total_loss / num_batches\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can go ahead and train the model on all of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 2.3009\n",
      "Epoch 2/10 - Loss: 2.1733\n",
      "Epoch 3/10 - Loss: 0.9216\n",
      "Epoch 4/10 - Loss: 0.5284\n",
      "Epoch 5/10 - Loss: 0.4272\n",
      "Epoch 6/10 - Loss: 0.3774\n",
      "Epoch 7/10 - Loss: 0.3350\n",
      "Epoch 8/10 - Loss: 0.2944\n",
      "Epoch 9/10 - Loss: 0.2592\n",
      "Epoch 10/10 - Loss: 0.2304\n"
     ]
    }
   ],
   "source": [
    "train_model(x_train, y_train, epochs=10, batch_size=32, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model\n",
    "\n",
    "Now that everything has been trained, we need to evaluate it on new data that it hasn't seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2150\n",
      "Test Accuracy: 93.82%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(x_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluates the trained model on the test dataset.\n",
    "    \n",
    "    x_test: Test images (flattened)\n",
    "    y_test: True labels (one-hot encoded)\n",
    "    \"\"\"\n",
    "    # Forward propagation on the entire test set\n",
    "    _, _, _, _, _, predictions = forward_propagation(x_test)\n",
    "\n",
    "    # Compute test loss\n",
    "    test_loss = compute_loss(y_test, predictions)\n",
    "\n",
    "    # Convert softmax outputs to class predictions\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "    true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = np.mean(predicted_labels == true_labels) * 100\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "test_accuracy = evaluate_model(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparamter Tuning\n",
    "\n",
    "Now that we have all the code to train and evaluate the model, we can try to tune the parameters to get better accuracy.\n",
    "\n",
    "There are a few strategies we could use:\n",
    "\n",
    "1. Manual Tuning - Pretty much just trial and error\n",
    "2. Grid Search - Try various combinations of parameters (Ex: Train models with learning_rate = [0.01, 0.001, 0.0001] and hidden_neurons = [64, 128, 256].)\n",
    "3. Random search: Randomly change parameters instead of trying all combinations\n",
    "\n",
    "Below, we make a function to test different hyperparameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(learning_rate=0.01, hidden_size1=128, hidden_size2=64, batch_size=32, epochs=10):\n",
    "    \"\"\"\n",
    "    Trains and evaluates the model with different hyperparameters.\n",
    "    \n",
    "    learning_rate: Step size for weight updates\n",
    "    hidden_size1: Number of neurons in first hidden layer\n",
    "    hidden_size2: Number of neurons in second hidden layer\n",
    "    batch_size: Number of samples processed at a time\n",
    "    epochs: Number of times the model sees the full dataset\n",
    "    \"\"\"\n",
    "    global W1, b1, W2, b2, W3, b3\n",
    "\n",
    "    # Print Hyperparameters\n",
    "    print(\"\\n=================================\")\n",
    "    print(\"Training with Hyperparameters:\")\n",
    "    print(f\"  Learning Rate: {learning_rate}\")\n",
    "    print(f\"  Hidden Layer 1 Neurons: {hidden_size1}\")\n",
    "    print(f\"  Hidden Layer 2 Neurons: {hidden_size2}\")\n",
    "    print(f\"  Batch Size: {batch_size}\")\n",
    "    print(f\"  Epochs: {epochs}\")\n",
    "    print(\"=================================\")\n",
    "\n",
    "    # Reinitialize Weights with New Hidden Layer Sizes\n",
    "    input_size = 784  # 28x28 pixels\n",
    "    output_size = 10  # 10 classes\n",
    "\n",
    "    np.random.seed(42)\n",
    "    W1 = np.random.randn(input_size, hidden_size1) * 0.01\n",
    "    b1 = np.zeros((1, hidden_size1))\n",
    "    W2 = np.random.randn(hidden_size1, hidden_size2) * 0.01\n",
    "    b2 = np.zeros((1, hidden_size2))\n",
    "    W3 = np.random.randn(hidden_size2, output_size) * 0.01\n",
    "    b3 = np.zeros((1, output_size))\n",
    "\n",
    "    # Train the Model\n",
    "    train_model(x_train, y_train, epochs, batch_size, learning_rate)\n",
    "\n",
    "    # Evaluate Model\n",
    "    test_accuracy = evaluate_model(x_test, y_test)\n",
    "\n",
    "    # Print Final Accuracy\n",
    "    print(f\"Final Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    return test_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets try some parameters randomly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================\n",
      "Training with Hyperparameters:\n",
      "  Learning Rate: 0.1\n",
      "  Hidden Layer 1 Neurons: 128\n",
      "  Hidden Layer 2 Neurons: 64\n",
      "  Batch Size: 32\n",
      "  Epochs: 10\n",
      "=================================\n",
      "Epoch 1/10 - Loss: 0.8316\n",
      "Epoch 2/10 - Loss: 0.1798\n",
      "Epoch 3/10 - Loss: 0.1163\n",
      "Epoch 4/10 - Loss: 0.0883\n",
      "Epoch 5/10 - Loss: 0.0713\n",
      "Epoch 6/10 - Loss: 0.0585\n",
      "Epoch 7/10 - Loss: 0.0483\n",
      "Epoch 8/10 - Loss: 0.0400\n",
      "Epoch 9/10 - Loss: 0.0334\n",
      "Epoch 10/10 - Loss: 0.0288\n",
      "Test Loss: 0.1036\n",
      "Test Accuracy: 97.09%\n",
      "Final Test Accuracy: 97.09%\n",
      "\n",
      "=================================\n",
      "Training with Hyperparameters:\n",
      "  Learning Rate: 0.001\n",
      "  Hidden Layer 1 Neurons: 128\n",
      "  Hidden Layer 2 Neurons: 64\n",
      "  Batch Size: 32\n",
      "  Epochs: 10\n",
      "=================================\n",
      "Epoch 1/10 - Loss: 2.3024\n",
      "Epoch 2/10 - Loss: 2.3019\n",
      "Epoch 3/10 - Loss: 2.3015\n",
      "Epoch 4/10 - Loss: 2.3012\n",
      "Epoch 5/10 - Loss: 2.3009\n",
      "Epoch 6/10 - Loss: 2.3007\n",
      "Epoch 7/10 - Loss: 2.3004\n",
      "Epoch 8/10 - Loss: 2.3001\n",
      "Epoch 9/10 - Loss: 2.2996\n",
      "Epoch 10/10 - Loss: 2.2991\n",
      "Test Loss: 2.2985\n",
      "Test Accuracy: 11.35%\n",
      "Final Test Accuracy: 11.35%\n",
      "\n",
      "=================================\n",
      "Training with Hyperparameters:\n",
      "  Learning Rate: 0.01\n",
      "  Hidden Layer 1 Neurons: 256\n",
      "  Hidden Layer 2 Neurons: 128\n",
      "  Batch Size: 32\n",
      "  Epochs: 10\n",
      "=================================\n",
      "Epoch 1/10 - Loss: 2.2915\n",
      "Epoch 2/10 - Loss: 1.4117\n",
      "Epoch 3/10 - Loss: 0.5883\n",
      "Epoch 4/10 - Loss: 0.4299\n",
      "Epoch 5/10 - Loss: 0.3576\n",
      "Epoch 6/10 - Loss: 0.3127\n",
      "Epoch 7/10 - Loss: 0.2760\n",
      "Epoch 8/10 - Loss: 0.2438\n",
      "Epoch 9/10 - Loss: 0.2166\n",
      "Epoch 10/10 - Loss: 0.1937\n",
      "Test Loss: 0.1796\n",
      "Test Accuracy: 94.82%\n",
      "Final Test Accuracy: 94.82%\n",
      "\n",
      "=================================\n",
      "Training with Hyperparameters:\n",
      "  Learning Rate: 0.01\n",
      "  Hidden Layer 1 Neurons: 128\n",
      "  Hidden Layer 2 Neurons: 64\n",
      "  Batch Size: 128\n",
      "  Epochs: 10\n",
      "=================================\n",
      "Epoch 1/10 - Loss: 2.3070\n",
      "Epoch 2/10 - Loss: 2.3061\n",
      "Epoch 3/10 - Loss: 2.3054\n",
      "Epoch 4/10 - Loss: 2.3044\n",
      "Epoch 5/10 - Loss: 2.3021\n",
      "Epoch 6/10 - Loss: 2.2932\n",
      "Epoch 7/10 - Loss: 2.2187\n",
      "Epoch 8/10 - Loss: 1.8961\n",
      "Epoch 9/10 - Loss: 1.3556\n",
      "Epoch 10/10 - Loss: 0.9380\n",
      "Test Loss: 0.7804\n",
      "Test Accuracy: 75.48%\n",
      "Final Test Accuracy: 75.48%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(75.48)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiment 1: Higher Learning Rate\n",
    "train_and_evaluate(learning_rate=0.1, hidden_size1=128, hidden_size2=64, batch_size=32, epochs=10)\n",
    "\n",
    "# Experiment 2: Lower Learning Rate\n",
    "train_and_evaluate(learning_rate=0.001, hidden_size1=128, hidden_size2=64, batch_size=32, epochs=10)\n",
    "\n",
    "# Experiment 3: More Hidden Neurons\n",
    "train_and_evaluate(learning_rate=0.01, hidden_size1=256, hidden_size2=128, batch_size=32, epochs=10)\n",
    "\n",
    "# Experiment 4: Larger Batch Size\n",
    "train_and_evaluate(learning_rate=0.01, hidden_size1=128, hidden_size2=64, batch_size=128, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced methods to improve accuracy\n",
    "\n",
    "There are some other methods to improve accuracy further. These include:\n",
    "\n",
    "1. Regularization - if the model does well on training data but not on test data, meaning that it's likely overfitting\n",
    "\n",
    "2. Dropout - randomly disabling neurons during training to prevent the model from relying too much on a subset of neurons\n",
    "\n",
    "3. Other optimization techniques to replace gradient descent such as [momentum](https://www.youtube.com/watch?v=k8fTYJPd3_I&ab_channel=DeepLearningAI) and the [Adam Optimizer](https://www.geeksforgeeks.org/adam-optimizer/) (used by default in torch and tensorflow)\n",
    "\n",
    "4. More advanced architectures such as CNNs\n",
    "\n",
    "5. Data augmentation - creating more data samples simply by modifying existing data, such as by transforming the image date through scaling, rotation, etc.\n",
    "\n",
    "6. Transfer learning - use a pretrained model and replace a few layers to adapt it to your application\n",
    "\n",
    "7. Model ensemble - Train multiple models with different architectures and average their predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "-1.-1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
